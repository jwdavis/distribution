{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Analysis with TensorFlow Data Validation and TensorFlow Model Analysis' Fairness Indicators\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/question_answering.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/question_answering.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/question_answering.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iVkPBosnIFlu"
   },
   "source": [
    "## Overview\n",
    "\n",
    "When releasing a machine learning model, it's critical to ensure its performance as well as its behavior. On top of traditional model evaluation, a responsible evaluation of a model requires assessing that data and model are fair, i.e. they avoid creating or reinforcing bias. Bias occurs when there is stereotyping, prejudice, or favoritism towards some things, people, or groups over others.\n",
    "\n",
    "In this notebook, you perform an initial fairness analysis on the [Civil Comments dataset](https://www.tensorflow.org/datasets/catalog/civil_comments) using two tools offered by TensorFlow for large-scale ML analysis:\n",
    "- [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) (TFDV) can analyze training and serving data to compute descriptive statistics, infer a schema, and detect data anomalies.<br>You use TFDV to find potential issues that can lead to fairness disparities, such as missing values and data imbalances.\n",
    "- [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/guide/tfma) (TFMA) can perform extensive model evaluation on training and test set, overall or for specific data slices, using a variety of metrics and visualizations. <br>TFMA provides easy access to [Fairness Indicators](https://www.tensorflow.org/tfx/guide/fairness_indicators): a tool designed to support teams in evaluating and improving models for fairness concerns. <br>You use Fairness Indicators to compute and visualize commonly-identified fairness metrics. \n",
    "\n",
    "Learn more about how Google applies fairness in ml at https://ai.google/responsibility/responsible-ai-practices/#fairness. \n",
    "\n",
    "## Objective\n",
    "\n",
    "By the end of the notebook, you should be able to:\n",
    "\n",
    "1. Use TFRecords to load record-oriented binary format data\n",
    "2. Use TFDV to generate statistics, and the TFDV widget to answer questions.\n",
    "3. Use TFMA with Fairness Indicators to calculate fairness metrics, and its API to programmatically access model analysis results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "First, you install TFDV and TFMA with some supporting packages.\n",
    "\n",
    "Then, you import the necessary dependencies for the libraries you'll be using in this exercise, which include TensorFlow Data Validation (TFDV), TensorFlow Model Analysis (TFMA), and Fairness Indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package requirements for this lab have been saved in the `requirements.txt` file. Versions of the TFDV and TFMA packages require specific versions of other packages such as TensorFlow and Apache Beam. You may run the following cell to see the contents of the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please only run the installation once. The following cell will take 3-5 minutes to install the proper versions of all of the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart the kernel\n",
    "After you install/upgrade the packages, you need to restart the notebook kernel so it can find those packages.\n",
    "\n",
    "Click **Kernel > Restart Kernel**, or uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs\n",
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     # Automatically restart kernel after installs\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6E__x2XkJDFW"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "from google.protobuf import text_format\n",
    "\n",
    "import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_model_analysis.addons.fairness.post_export_metrics import fairness_indicators\n",
    "\n",
    "from tfx_bsl.tfxio import tf_example_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data using TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3R2QWkru1WN",
    "tags": []
   },
   "source": [
    "### About the Civil Comments dataset\n",
    "\n",
    "Click below to learn more about the Civil Comments dataset, and the pre-processing it has underfone for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZswcJJMCDjU",
    "tags": []
   },
   "source": [
    "#### Overview\n",
    "\n",
    "The Civil Comments dataset comprises approximately 2 million public comments that were submitted to the Civil Comments platform. [Jigsaw](https://jigsaw.google.com/) sponsored the effort to compile and annotate these comments for ongoing [research](https://arxiv.org/abs/1903.04561); they've also hosted competitions on [Kaggle](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) to help classify toxic comments as well as minimize unintended model bias. \n",
    "\n",
    "##### Features\n",
    "\n",
    "Within the Civil Comments data, a subset of comments are tagged with a variety of *identity* attributes pertaining to gender, sexual orientation, religion, race, and ethnicity. \n",
    "\n",
    "**NOTE:** These identity attributes are intended *for evaluation purposes only*, to assess how well a classifier trained solely on the comment text performs on different tag sets.\n",
    "\n",
    "To collect these identity labels, each comment was reviewed by up to 10 annotators, who were asked to indicate all identities that were mentioned in the comment. For example, annotators were posed the question: \"What genders are mentioned in the comment?\", and asked to choose all of the following categories that were applicable.\n",
    "\n",
    "* Male\n",
    "* Female\n",
    "* Transgender\n",
    "* Other gender\n",
    "* No gender mentioned\n",
    "\n",
    "**NOTE:** *We recognize the limitations of the categories used in the original dataset, and acknowledge that these terms do not encompass the full range of vocabulary used in describing gender.*\n",
    "\n",
    "Jigsaw used these ratings to generate an aggregate score for each identity attribute representing the percentage of raters who said the identity was mentioned in the comment. For example, if 10 annotators reviewed a comment, and 6 said that the comment mentioned the identity \"female\" and 0 said that the comment mentioned the identity \"male,\" the comment would receive a `female` score of `0.6` and a `male` score of `0.0`.\n",
    "\n",
    "**NOTE:** For the purposes of annotation, a comment was considered to \"mention\" gender if it contained a comment about gender issues (e.g., a discussion about feminism, wage gap between men and women, transgender rights, etc.), gendered language, or gendered insults. Use of \"he,\" \"she,\" or gendered names (e.g., Donald, Margaret) did not require a gender label. \n",
    "\n",
    "##### Label\n",
    "\n",
    "Each comment was rated by up to 10 annotators for toxicity, who each classified it with one of the following ratings.\n",
    "\n",
    "* Very Toxic\n",
    "* Toxic\n",
    "* Hard to Say\n",
    "* Not Toxic\n",
    "\n",
    "Again, Jigsaw used these ratings to generate an aggregate toxicity \"score\" for each comment (ranging from `0.0` to `1.0`) to serve as the [label](https://developers.google.com/machine-learning/glossary?utm_source=Colab&utm_medium=fi-colab&utm_campaign=fi-practicum&utm_content=glossary&utm_term=label#label), representing the fraction of annotators who labeled the comment either \"Very Toxic\" or \"Toxic.\" For example, if 10 annotators rated a comment, and 3 of them labeled it \"Very Toxic\" and 5 of them labeled it \"Toxic\", the comment would receive a toxicity score of `0.8`.\n",
    "\n",
    "**NOTE:** For more information on the Civil Comments labeling schema, see the [Data](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data) section of the Jigsaw Untended Bias in Toxicity Classification Kaggle competition.\n",
    "\n",
    "\n",
    "#### Preprocessing the data\n",
    "For the purposes of this exercise, the data has been pre-processed as follows:\n",
    "    \n",
    "- the colum *toxicity*, i.e. the label, has been converted to a binary label: any value â¥ 0.5 is labeled as 1, i.e. true (i.e., a comment is considered toxic if 50% or more crowd raters labeled it as toxic); vice versa, any value <-.5 is labeled as 0, i.e. false.\n",
    "    \n",
    "- the column *identity* has been grouped by its categories [*gender*, *sexual_orientation*, *disability*, *religion*, *race*] , with a value assigned to the category using a threshold of 0.5. For example, if one comment has `{ male: 0.3, female: 1.0, transgender: 0.0, heterosexual: 0.8, homosexual_gay_or_lesbian: 1.0 }`, data is transformed into `{ gender: [female], sexual_orientation: [heterosexual, homosexual_gay_or_lesbian] }`.\n",
    "\n",
    "Finally, the *comment_text* column reports the text of the individual comment as is.\n",
    "\n",
    "**NOTE:** Missing identity fields were converted to False.\n",
    "\n",
    "**NOTE:** Fairness Indicators currently work with binary and multiclass classification only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0YNqAJW5JjZD"
   },
   "source": [
    "### Use TFRecords to load record-oriented binary format data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "The [TFRecord format](https://www.tensorflow.org/tutorials/load_data/tfrecord) is a simple [Protobuf](https://developers.google.com/protocol-buffers)-based format for storing a sequence of binary records. It gives you and your machine learning models the ability to handle arbitrarily large datasets (that don't fit in memory!) over the network by:\n",
    "1. Splitting up large files into 100-200MB chunks\n",
    "2. Storing the results as serialized binary messages for faster ingestion\n",
    "\n",
    "Let's access the publicly available training and validation sets for the pre-processed Civil Comments dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duPWGTQAvYKK"
   },
   "outputs": [],
   "source": [
    "train_tf_file = tf.keras.utils.get_file(\n",
    "    'train_tf_processed.tfrecord',\n",
    "    'https://storage.googleapis.com/civil_comments_dataset/train_tf_processed.tfrecord'\n",
    ")\n",
    "validate_tf_file = tf.keras.utils.get_file(\n",
    "    'validate_tf_processed.tfrecord',\n",
    "    'https://storage.googleapis.com/civil_comments_dataset/validate_tf_processed.tfrecord'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze fairness with TFDV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLup7wY0_Q3K"
   },
   "source": [
    "### Use TFDV to generate and visualize statistics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TensorFlow Data Validation enables you to calculate data statistics automatically.\n",
    "\n",
    "For tabular data, it supports:\n",
    "- data stored in a TFRecord file\n",
    "- data stored in a CSV input format\n",
    "- data loaded in-memory in a Pandas dataframe\n",
    "\n",
    "And you can also create your own [custom data connector](https://www.tensorflow.org/tfx/data_validation/get_started#writing_custom_data_connector).\n",
    "\n",
    "Before you train the model, you want to do an audit of the data so to better understand data distributions and analyze the presence of unexpected values. \n",
    "<br> Let's use the [tf.generate_statistics_from_tfrecord()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_tfrecord) functionality to generate statistics overall for the data.\n",
    "\n",
    "**NOTE:** *It could take up to 1 minute to generate the statistics.*\n",
    "\n",
    "**FUN FACT:** *TFDV can also compute statistics for semantic domains (e.g., images, text). To enable computation of semantic domain statistics, pass a tfdv.StatsOptions object with enable_semantic_domain_stats set to True to tfdv.generate_statistics_from_tfrecord.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use TFDV to generate and visualize statistics overall for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkzcE_g8_m_h"
   },
   "outputs": [],
   "source": [
    "# Compute the statistics for the training set\n",
    "stats = tfdv.generate_statistics_from_tfrecord(data_location=train_tf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TFDV to infer and analyze the data schema\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TFDV enables you to automatically infer the schema given calculated statistics, and visualize it. \n",
    "\n",
    "Let's use the [tfdv.infer_schema()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) functionality to get familiar with the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer and visalize the schema\n",
    "schema = tfdv.infer_schema(stats)\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the schema, you can see that you have 7 features, all required. \n",
    "\n",
    "There are 6 categorical features, which are: 'comment_text', 'disability', 'gender', 'race', 'religion', and 'sexual_orientation'.\n",
    "You can get a preview of the values of each categorical feature.\n",
    "\n",
    "There is 1 numerical feature, which is: 'toxicity'. This is the label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TFDV to visualize and analyze the data statistics \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Let's use the [tfdv.visualize_statistics()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics) functionality to visualize the generated statistics for the training set, and perform some analysis.\n",
    "\n",
    "**FUN FACT:** *You can use **tfdv.visualize_statistics()** to visualize training and test set statistics side-by-side too! Other parameters include an allow list of features and a deny list of features to visualize.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the statistics for analysis\n",
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZU1Djze6E-s"
   },
   "source": [
    "**What can you learn from the visualizations of the statistics?** \n",
    "\n",
    "From this  visualization, you can gather a lot of interesting information as you can see all of the features with statistics and data distributions in one view. \n",
    "\n",
    "For example, you can  see that there are no missing values. In fact, the highlighted 92.08% missing values for the numerical column 'toxicity' is expected since the label is 0 for non-toxicity and 1 for toxicity.\n",
    "\n",
    "The widget provides slighly different statistics for numerical and categorical values:\n",
    "- For numericals, we get: count, missing, mean, std dev, zeroes, min, median, max.\n",
    "- For categoricals, we get: count, missing, unique, top, freq top, avg str len.\n",
    "\n",
    "\n",
    "Spend some time exploring the generated stats, and the information they provide you with.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use TFDV to generate and visualize statistics for subset groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TensorFlow Data Validation enables you to calculate and visualize data statistics for specific subset groups, known as *data slices*. You can do so by adding the [tfdv.StatsOption](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/StatsOptions) parameter to the [tfdv.generate_statistics_from_tfrecord()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_tfrecord) functionality. \n",
    "\n",
    "Let's generate statistics using the feature \"gender\" as a data slice.\n",
    "\n",
    "\n",
    "**NOTE:** *It could take a couple of minutes to generate the statistics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define slice\n",
    "slice_fn = tfdv.experimental_get_feature_value_slicer(\n",
    "    features={'gender': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistics from slice\n",
    "stats_options = tfdv.StatsOptions(experimental_slice_functions=[slice_fn])\n",
    "sliced_stats = tfdv.generate_statistics_from_tfrecord(\n",
    "    data_location=train_tf_file,\n",
    "    stats_options=stats_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: *The [visualize_statistics()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics) method currently only supports comparing up to two datasets. Let's get the name of the statistics datasets created by slicing, and compare the two datasets for all examples and for the female gender.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the statistics datasets created by slicing\n",
    "for dataset in sliced_stats.datasets:\n",
    "    print(dataset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the \"All Examples\" dataset, i.e. the same we created before, and the \"gender_female\" dataset\n",
    "tfdv.visualize_statistics(\n",
    "    lhs_statistics=tfdv.get_slice_stats(sliced_stats, 'All Examples'),\n",
    "    rhs_statistics=tfdv.get_slice_stats(sliced_stats, 'gender_female'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What important difference can you spot between the statistics for the overall dataset and by female-gender slice?**\n",
    "\n",
    "The statistics from two features differ between the two datasets:\n",
    "\n",
    "1. `comment_text`: the top comment is different, and the fact that it is a gender-biased comment for females may be problematic; as a next step, you could analyze the correlation between this feature and the label column, *overall* and by the *gender=female* data slice, to see if there are any differences. \n",
    "\n",
    "2. `toxicity`: the percentage of gender-related examples that are toxic (100-86.3=13.7%) is nearly double the percentage of toxic examples overall (100-92.02=7.98%). \n",
    "<br> In other words, comments related to the female gender are almost two times more likely than comments overall to be labeled as toxic. This skew suggests that a model trained on this dataset might learn a correlation between gender-related content and toxicity. This raises fairness considerations, as the model might be more likely to classify nontoxic comments as toxic if they contain gender terminology, which could lead to [disparate impact](https://developers.google.com/machine-learning/glossary?utm_source=Colab&utm_medium=fi-colab&utm_campaign=fi-practicum&utm_content=glossary&utm_term=disparate-impact#disparate-impact) for gender subgroups. \n",
    "\n",
    "If you have time, why not try to run the same analysis for the other *gender* data slices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze fairness with TFMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Learn about the pre-trained model\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "For this exercise, you want to analyze the performance of a trained model for fairness.\n",
    "\n",
    "You can access a simple pre-trained binary classification model on the pre-processed Civic Dataset in the folder **saved_model/**.\n",
    "\n",
    "The model is a deep neural network trained on all features analyzed in the TFDV section: the \"comment_text\" feature is embedded using the https://tfhub.dev/google/nnlm-en-dim128/1 model, and the other categorical features are set as variable-length strings. The model has two hidden layers with 500 and 100 neurons, and uses the Adagrad optimizer with the loss reduced by summing. To handle the unbalanced classes, class weights are added to the training. The model is trained for 1000 steps with batch size of 512. The trained model has been exported to the saved_model format for serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model analysis with TFMA's Fairness Indicators on the validation set\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TFMA allows you to run model analysis on a trained / serving model using the [run_model_analysis()](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/run_model_analysis) functionality. \n",
    "\n",
    "On top of traditional metrics, TFMA provides access to the [Fairness Indicators](https://www.tensorflow.org/tfx/guide/fairness_indicators) library which enables easy computation of common fairness metrics--grouped inside the *FairnessIndicators* metric--at scale.\n",
    "\n",
    "You need to define an evaluation configuration to specify the evaluation you want to perform; the eval_config should include:\n",
    "- model_specs to define the column names for example labels and (optional) predictions.\n",
    "- metrics_specs to define the metrics to compute. The FairnessIndicators metric will be required to render the fairness metrics and you can see a list of additional optional metrics [here](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics).\n",
    "- slicing_specs to optionally define what feature(s) youâre interested in investigating. More than one slice can be provided, and if the slicing spec is left empty then all featuers are analyzed.\n",
    "\n",
    "Let's analyze the model's performance on the validation at multiple thresholds. \n",
    "\n",
    "**NOTE:** *It could take up to 10-15 minutes to generate the evaluation results.*\n",
    "\n",
    "**FUN FACT:** *You can compare up to two models!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "tfma_export_dir = \"saved_model\"\n",
    "tfma_eval_result_path = 'tfma_eval_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation config\n",
    "eval_config = text_format.Parse(\"\"\"\n",
    "    model_specs {\n",
    "      label_key: \"toxicity\"\n",
    "    }\n",
    "    metrics_specs {\n",
    "      metrics {\n",
    "        class_name: \"FairnessIndicators\"\n",
    "        config: '{ \"thresholds\": [0.1, 0.3, 0.5, 0.7, 0.9] }'\n",
    "      }\n",
    "    }\n",
    "    slicing_specs {}  # overall slice\n",
    "    slicing_specs {feature_keys: [\"gender\"]}  # we can slice by any feature\n",
    "    \n",
    "    options {\n",
    "        compute_confidence_intervals { value: False }  # we can optionally compute CIs for a more detailed analysis\n",
    "    }\n",
    "\"\"\", tfma.EvalConfig())\n",
    "\n",
    "# Load the pre-trained model to evaluate\n",
    "eval_shared_model = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path=tfma_export_dir,\n",
    "    eval_config=eval_config,\n",
    ")\n",
    "\n",
    "# Run the analysis\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model,\n",
    "    eval_config=eval_config,\n",
    "    data_location=validate_tf_file,\n",
    "    output_path=tfma_eval_result_path,\n",
    "    random_seed_for_testing=42,  # a random seed ensures deterministic results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model analysis with TFMA's Fairness Indicators at scale with Apache Beam\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[Apache Beam](https://beam.apache.org/releases/pydoc/2.6.0/index.html) provides a simple and powerful programming model for building both batch and streaming parallel data processing pipelines.\n",
    "\n",
    "One of the unique capabilities of TFMA, as well as TFDV, is the ability to perform large-scale distributed computations using [Apache Beam](https://beam.apache.org/releases/pydoc/2.6.0/index.html).\n",
    "\n",
    "You can explore the code below that generates the same evaluation results as with *tfma.run_model_analysis()* using tfma with Beam. \n",
    "\n",
    "We suggest skipping running this code here for the sake of time. \n",
    "\n",
    "```python\n",
    "# Access the validation set\n",
    "tfx_io = tf_example_record.TFExampleRecord(\n",
    "    file_pattern=validate_tf_file,\n",
    "    raw_record_column_name=tfma.ARROW_INPUT_COLUMN)\n",
    "\n",
    "# Perform the model analysis evaluation\n",
    "with beam.Pipeline() as pipeline:\n",
    "    _ = (\n",
    "        pipeline\n",
    "        | 'Read TFRecords' >> tfx_io.BeamSource()\n",
    "        | 'Perform and Save Model Analysis' >> tfma.ExtractEvaluateAndWriteResults(\n",
    "            eval_config=eval_config,\n",
    "            eval_shared_model=eval_shared_model,\n",
    "            output_path=tfma_eval_result_path,\n",
    "            random_seed_for_testing=42,\n",
    "        )\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get fairness evaluation results programmatically\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TFMA lets you interactively visualize results using the [render_slicing_metrics()](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_slicing_metrics) functionality; unfortunately, there is currently a bug in the rendering for some Jupyter environments including Vertex AI Workbench.\n",
    "\n",
    "Visualizing results is important for exploration, whila accessing results programmatically is important for monitoring and automation.\n",
    "\n",
    "The [EvalResult](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalResult) object returned by TFMA's evaluation has its own API that you can leverage to read TFMA results into your programs. \n",
    "\n",
    "Let's use the API to access the fairness evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output prettifier\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of slice names\n",
    "print(\"Slices:\")\n",
    "pp.pprint(eval_result.get_slice_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluated metrics overall for the dataset\n",
    "print(\"\\nMetrics:\")\n",
    "pp.pprint(eval_result.get_metric_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluated metrics for a particular slice, and compare it to a baseline slice composed of all data\n",
    "baseline_slice = ()\n",
    "female_slice = (('gender', 'female'),)\n",
    "\n",
    "print(\"Baseline metric values:\")\n",
    "pp.pprint(eval_result.get_metrics_for_slice(baseline_slice))\n",
    "print(\"\\Gender metric values:\")\n",
    "pp.pprint(eval_result.get_metrics_for_slice(female_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics for all data slices at once\n",
    "pp.pprint(eval_result.get_metrics_for_all_slices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "J3R2QWkru1WN",
    "UFBqqnRD-Zkj",
    "6KmrCS-uAz0s",
    "IvvxNMgM-6A2",
    "-J4hbOhgHZid",
    "tGyACRd8oFwP",
    "FQGWSdrJy08B",
    "LlkfgynX0yfF",
    "FBhBsevUOinO",
    "P5MBQR7EF6ny",
    "OaL3qgHCcmwG"
   ],
   "name": "Copy of Fairness Exercise 1: Explore the Model",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "tfdv_tfma",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
